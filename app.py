from __future__ import annotations

import os
import time
import pandas as pd
import streamlit as st

from src.config import load_config
from src.data_loader import load_examples
from src.llm import LLMClient
from src.memory import MemoryTurn, trim_memory
from src.preprocess import normalize_question
from src.prompt import build_prompt, rewrite_question
from src.retrieval import HybridRetriever
from src.schema import get_schema
from src.sql_executor import execute_sql

st.set_page_config(page_title="Text2SQL æ™ºèƒ½é—®æ•°ç³»ç»Ÿ", layout="wide")

st.markdown("""
<style>
    .main { background-color: #f8f9fa; }
    .stChatMessage { border-radius: 15px; padding: 10px; margin-bottom: 10px; max-width: 85%; }
    div[data-testid="stChatMessageUser"] { margin-left: auto; background-color: #e3f2fd; border: 1px solid #bbdefb; }
    div[data-testid="stChatMessageAssistant"] { margin-right: auto; background-color: #ffffff; border: 1px solid #e0e0e0; box-shadow: 0 2px 4px rgba(0,0,0,0.05); }
    .stStatus { border-radius: 10px; }
    .block-container { padding-top: 2rem; }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ¤– Text2SQL æ™ºèƒ½é—®æ•°ç³»ç»Ÿ")

config = load_config()

if "chat" not in st.session_state: st.session_state["chat"] = []
if "memory" not in st.session_state: st.session_state["memory"] = []

with st.sidebar:
    st.subheader("âš™ï¸ æ¨¡å‹é…ç½®")
    model_name = st.text_input("æ¨¡å‹åç§°", value=config.model_name)
    base_url = st.text_input("Base URL", value=config.base_url or "")
    api_key = st.text_input("API Key", value=config.api_key or "", type="password")
    temperature = st.slider("Temperature", 0.0, 1.0, config.temperature, 0.05)
    top_k = st.slider("ç¤ºä¾‹æ•°é‡ (Top-K)", 0, 10, config.top_k_examples)
    memory_turns = st.slider("è®°å¿†è½®æ•°", 0, 10, 3)

    st.subheader("ğŸ“‚ æ•°æ®è·¯å¾„")
    db_path = st.text_input("SQLite DB", value=config.db_path)
    enable_chart = st.checkbox("å¯ç”¨å›¾è¡¨å¯è§†åŒ–", value=True)

    st.subheader("ğŸ§¹ è®°å¿†ç®¡ç†")
    if st.button("æ¸…ç©ºè®°å¿†"): st.session_state["memory"] = []
    if st.button("æ¸…ç©ºå¯¹è¯è®°å½•"):
        st.session_state["chat"] = []
        st.session_state["last_prompt"] = None
        st.rerun()

@st.cache_resource(show_spinner=False)
def _load_resources(train_path: str, db_path_val: str):
    schema = get_schema(db_path_val)
    examples = load_examples(train_path, "college_2")
    retriever = HybridRetriever(examples)
    return schema, retriever

schema_text, retriever = _load_resources(os.path.join(config.data_root, "train.json"), db_path)

col_left, col_right = st.columns([2, 1])

with col_left:
    st.subheader("ğŸ’¬ æ™ºèƒ½å¯¹è¯")
    chat_placeholder = st.container()
    
    if prompt_input := st.chat_input("è¯·è¾“å…¥è‡ªç„¶è¯­è¨€é—®é¢˜..."):
        normalized = normalize_question(prompt_input)
        st.session_state["chat"].append({"role": "user", "content": normalized})
        
        llm = LLMClient(model_name=model_name, api_key=api_key or None, base_url=base_url or None, temperature=temperature)
        history = trim_memory(st.session_state["memory"], memory_turns)

        with st.status("ğŸš€ æ™ºèƒ½ä½“æ­£åœ¨æ€è€ƒ...", expanded=True) as status:
            # 1. é‡å†™
            target_q = normalized
            if history:
                st.write("ğŸ”„ æ­£åœ¨åˆ†æä¸Šä¸‹æ–‡...")
                rewritten = rewrite_question(llm, normalized, history)
                if rewritten != normalized:
                    st.write(f"ğŸ“ é‡å†™é—®é¢˜: **{rewritten}**")
                    target_q = rewritten
            
            # 2. æ£€ç´¢
            st.write("ğŸ” æ­£åœ¨æ£€ç´¢æ··åˆç¤ºä¾‹...")
            few_shot = retriever.search(target_q, k=top_k)
            
            # 3. ç”Ÿæˆ
            st.write("ğŸ¤– æ­£åœ¨ç”Ÿæˆ SQL...")
            full_prompt = build_prompt(schema_text, few_shot, target_q, history)
            st.session_state["last_prompt"] = full_prompt
            st.session_state["last_example_count"] = len(few_shot)
            
            start_time = time.time()
            sql = llm.generate_sql(full_prompt)
            latency = time.time() - start_time
            
            if not sql.strip().lower().startswith("select"):
                status.update(label="âš ï¸ æœªèƒ½ç”Ÿæˆæœ‰æ•ˆæŸ¥è¯¢", state="error")
                st.session_state["chat"].append({"role": "assistant", "content": f"æœªèƒ½ç”Ÿæˆæœ‰æ•ˆ SQLã€‚LLM è¾“å‡ºï¼š\n\n```\n{sql}\n```"})
            else:
                st.write("âš¡ æ­£åœ¨æ‰§è¡ŒæŸ¥è¯¢...")
                try:
                    try:
                        result = execute_sql(db_path, sql)
                    except Exception as e:
                        # Execution-guided self-correction (retry once)
                        fixed_sql = llm.repair_sql(full_prompt, sql, str(e))
                        result = execute_sql(db_path, fixed_sql)
                        sql = fixed_sql
                    status.update(label=f"âœ… å®Œæˆ (è€—æ—¶ {latency:.2f}s)", state="complete")
                    st.session_state["chat"].append({
                        "role": "assistant",
                        "content": f"å·²ç”Ÿæˆ SQLï¼š\n```sql\n{sql}\n```\næŸ¥è¯¢åˆ° {result.row_count} æ¡ç»“æœã€‚",
                        "result": result
                    })
                    st.session_state["memory"].append(MemoryTurn(question=target_q, sql=sql))
                    st.session_state["memory"] = trim_memory(st.session_state["memory"], memory_turns)
                except Exception as e:
                    status.update(label="âŒ æ‰§è¡Œå‡ºé”™", state="error")
                    st.session_state["chat"].append({"role": "assistant", "content": f"SQL æ‰§è¡Œå‡ºé”™ï¼š{str(e)}\n\nSQLï¼š\n```sql\n{sql}\n```"})

    with chat_placeholder:
        for i, msg in enumerate(st.session_state["chat"]):
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])
                if "result" in msg:
                    res = msg["result"]
                    df = pd.DataFrame(res.rows, columns=res.columns)
                    with st.expander("ğŸ“Š æ•°æ®è¯¦æƒ…ä¸å¯è§†åŒ–", expanded=(i == len(st.session_state["chat"])-1)):
                        st.dataframe(df, use_container_width=True)
                        if enable_chart and not df.empty:
                            for col in df.columns: df[col] = pd.to_numeric(df[col], errors="ignore")
                            num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
                            if num_cols:
                                st.divider()
                                c_type = st.selectbox(f"å›¾è¡¨ç±»å‹_{i}", ["æŸ±çŠ¶å›¾", "æŠ˜çº¿å›¾", "é¥¼å›¾", "æ•£ç‚¹å›¾"], key=f"t_{i}")
                                x_c = st.selectbox(f"Xè½´ (ç»´åº¦)_{i}", list(df.columns), index=0, key=f"x_{i}")
                                y_c = st.selectbox(f"Yè½´ (æŒ‡æ ‡)_{i}", num_cols, index=0, key=f"y_{i}")
                                try:
                                    import plotly.express as px
                                    if c_type == "æŸ±çŠ¶å›¾": fig = px.bar(df, x=x_c, y=y_c)
                                    elif c_type == "æŠ˜çº¿å›¾": fig = px.line(df, x=x_c, y=y_c)
                                    elif c_type == "é¥¼å›¾": fig = px.pie(df, names=x_c, values=y_c)
                                    else: fig = px.scatter(df, x=x_c, y=y_c)
                                    st.plotly_chart(fig, use_container_width=True)
                                except Exception as e: st.error(f"ç»˜å›¾å¤±è´¥: {e}")

with col_right:
    st.subheader("ğŸ“‹ æ•°æ®åº“ Schema")
    st.code(schema_text, language="sql")
    if st.session_state.get("last_prompt"):
        with st.expander("ğŸ” ä¸Šæ¬¡ç”Ÿæˆçš„ Prompt"):
            st.caption(f"æ£€ç´¢åˆ° {st.session_state.get('last_example_count', 0)} æ¡ Few-shot ç¤ºä¾‹")
            st.code(st.session_state["last_prompt"])
